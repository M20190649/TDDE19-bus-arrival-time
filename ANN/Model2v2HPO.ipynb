{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, still in the process of experimenting with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pds\n",
    "from tensorflow import keras\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from tensorflow.keras.optimizers import Adadelta, Nadam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(19, input_shape=(19,)))\n",
    "    model.add(Dense({{choice([2,8])}}, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='mae', \n",
    "                  optimizer={{choice(['RMSProp', 'Adadelta'])}},\n",
    "                  metrics=['mae', 'mape']\n",
    "                 )\n",
    "    \n",
    "    #result = model.fit(x_train,\n",
    "    #                  y_train,\n",
    "    ##                  epochs = 5,\n",
    "    #                  batch_size=10,\n",
    "    #                  validation_data=[x_test, y_test])\n",
    "    result = model.fit(x_train, y_train,\n",
    "                   batch_size={{choice([64, 128])}},\n",
    "                   epochs=2,\n",
    "                   verbose=0,\n",
    "                   validation_split=0.1)\n",
    "    \n",
    "    loss_metric = np.amax(result.history['mean_absolute_error']) \n",
    "    #print('Current epoch:', result.history['mean_absolute_error'], result.history['mean_absolute_percentage_error'])\n",
    "    return {'loss': loss_metric, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    data = pds.read_pickle('dataset_model2v2_w_label.pkl')\n",
    "    dataset = data.iloc[0:10000]\n",
    "    labels = dataset['label']\n",
    "    dataset = dataset.drop(columns=['label'])\n",
    "\n",
    "    num_test_segments = 3\n",
    "    msk = np.random.randint(1, dataset.journey_number.unique()[-1], num_test_segments)\n",
    "    mask = dataset['journey_number'].isin(msk)\n",
    "\n",
    "    train_input = dataset[~mask]\n",
    "    test_input = dataset[mask]\n",
    "\n",
    "    train_labels = labels[~mask]\n",
    "    test_labels = labels[mask]\n",
    "\n",
    "    train_data = train_input.drop(columns=['journey_number'])\n",
    "    test_data = test_input.drop(columns=['journey_number'])\n",
    "\n",
    "    x_train = train_input.drop(columns=['journey_number'])\n",
    "    x_test = test_data\n",
    "    y_train = train_labels\n",
    "    y_test = test_labels\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pds\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.optimizers import Adadelta, Nadam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [2,8]),\n",
      "        'optimizer': hp.choice('optimizer', ['RMSProp', 'Adadelta']),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: data = pds.read_pickle('dataset_model2v2_w_label.pkl')\n",
      "  3: dataset = data.iloc[0:10000]\n",
      "  4: labels = dataset['label']\n",
      "  5: dataset = dataset.drop(columns=['label'])\n",
      "  6: \n",
      "  7: num_test_segments = 3\n",
      "  8: msk = np.random.randint(1, dataset.journey_number.unique()[-1], num_test_segments)\n",
      "  9: mask = dataset['journey_number'].isin(msk)\n",
      " 10: \n",
      " 11: train_input = dataset[~mask]\n",
      " 12: test_input = dataset[mask]\n",
      " 13: \n",
      " 14: train_labels = labels[~mask]\n",
      " 15: test_labels = labels[mask]\n",
      " 16: \n",
      " 17: train_data = train_input.drop(columns=['journey_number'])\n",
      " 18: test_data = test_input.drop(columns=['journey_number'])\n",
      " 19: \n",
      " 20: x_train = train_input.drop(columns=['journey_number'])\n",
      " 21: x_test = test_data\n",
      " 22: y_train = train_labels\n",
      " 23: y_test = test_labels\n",
      " 24: \n",
      " 25: \n",
      " 26: \n",
      " 27: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:         \n",
      "   4:     model = Sequential()\n",
      "   5:     \n",
      "   6:     model.add(Dense(19, input_shape=(19,)))\n",
      "   7:     model.add(Dense(space['Dense'], activation='relu'))\n",
      "   8:     model.add(Dense(1, activation='sigmoid'))\n",
      "   9: \n",
      "  10:     model.compile(loss='mae', \n",
      "  11:                   optimizer=space['optimizer'],\n",
      "  12:                   metrics=['mae', 'mape']\n",
      "  13:                  )\n",
      "  14:     \n",
      "  15:     #result = model.fit(x_train,\n",
      "  16:     #                  y_train,\n",
      "  17:     ##                  epochs = 5,\n",
      "  18:     #                  batch_size=10,\n",
      "  19:     #                  validation_data=[x_test, y_test])\n",
      "  20:     result = model.fit(x_train, y_train,\n",
      "  21:                    batch_size=space['batch_size'],\n",
      "  22:                    epochs=2,\n",
      "  23:                    verbose=0,\n",
      "  24:                    validation_split=0.1)\n",
      "  25:     \n",
      "  26:     loss_metric = np.amax(result.history['mean_absolute_error']) \n",
      "  27:     #print('Current epoch:', result.history['mean_absolute_error'], result.history['mean_absolute_percentage_error'])\n",
      "  28:     return {'loss': loss_metric, 'status': STATUS_OK, 'model': model}\n",
      "  29: \n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name=\"Model2v2HPO\"\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense': 0, 'batch_size': 0, 'optimizer': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa11826d518>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py venv",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
