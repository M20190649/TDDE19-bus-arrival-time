
\chapter{Method}
\label{cha:method}

\section{Pre-processing}\label{pre-processing}
\label{sec:pre-processing}
The complete dataset that was delivered by Östgötatrafiken was over 300GB in size, comprising of over half a billion observations. One observation in the data contains timestamp, GPS coordinates, speed, angle of the bus, and  event specific information representing actions performed by a bus. 

Since this data was unstructured in many ways, there was a need to transform it into a format that supports querying. By iterating over and parsing the raw log files, the relevant subset of the data could be entered into Pandas\footnote{https://pandas.pydata.org/} data frames. Operations such as filtering and plotting the statistical distributions of the raw data provided an initial overview of the dataset. This would, in turn, facilitate the creation of a simple finite state machine, which could extract data covering a complete journey from a specific bus line. This "clean" data could then be used as input to train our models.

\subsection{Data structure}
The provided data consists of one file per day, with a total of 92 days between February and April in 2018. Each file has an approximate size of 5 GB, with around 2.4 million observations per file. Within these files are logs over the \textit{events}, which each bus sends throughout the day. The events represent different states of the bus, and there are over 20 different event-types. For this project, only four event-types were needed to successfully extract complete journeys that could be used as input for our models. These four events and their usage are listed below:\\
\begin{description}
\item[ObservedPositionEvent:] Is sent by buses with a frequency of 1Hz. This event contains the GPS data, speed, and direction (angle) of a given bus.
\item[EnteredEvent:] Is triggered when the bus is within a certain distance to a bus station. This event is used to split a journey into segments.
\item[JourneyStartedEvent:] Is triggered when the bus is assigned a new journey. This event is used to determine which line a bus is currently serving.
\item[JourneyCompletedEvent:] Is triggered when the bus has completed a journey. This event is used as a flag to determine when a journey has ended.
\end{description}

For this project, two bus lines were selected: Bus 3 and Bus 11. For both lines, only the journeys going in one direction were used. For line three, a subset of the segments from an intermediate bus stop until the last bus stop was selected. The reason for not using the complete journeys (i.e. from the first to the last bus stop) is because there was construction work being performed on some bus stops during the time the data was collected. This caused buses to take different routes which would be difficult to encapsulate in our models.

Line three was chosen because the authors were familiar with this line, and knew its irregularities during the time period. This line also had little noise in the GPS signal due to the absence of tall buildings along the route. Conversely, line eleven (seen in Figure \ref{fig:211_stations}) was chosen because it captures many problems that need to be taken into consideration within our predictions.  In contrast to line three, line eleven includes more GPS noise due to nearby tall buildings as well as a higher density of red lights and more traffic congestion.

\begin{figure}[!b]
\begin{minipage}{.5\textwidth}
    \includegraphics[scale=0.48,width=\textwidth]{211_stations}
    \caption{This figure shows a complete journey of the Bus 11. The markers in green are events of type EnteredEvent. These events are triggered when the bus reaches within a certain distance of a station, and have been used to divide journeys into segments.}
    \label{fig:211_stations}
\end{minipage}
\hspace{5pt}
\begin{minipage}{.48\textwidth}
\includegraphics[scale=0.5,width=\textwidth]{entered}
\caption{In this figure, the bus passed the station in the upper left of the figure without stopping. Since the events for \textit{stopping} at a station doesn't get fired then (the black marker in the lower right of the figure), the EnteredEvent is used instead.}
\label{fig:entered}
\end{minipage}
\end{figure}


\subsection{Detecting Segments}
Bus journeys are split into several segments. The \textit{EnteredEvent} is used to detect when a bus is approaching a station and is also used to split the raw data into segments. The first approach was to use the \textit{StoppedEvent} for segment detection, but as seen in Figure \ref{fig:entered}, that event is not suitable. This is because this event "skips" some stations since there are no people trying to get on or off the bus.

The first segment of a journey starts when a \textit{JourneyStartedEvent} triggers. When all the journeys of interest have been collected from the raw data, the collection is investigated in order to find and remove severe anomalies (such as drivers taking a wrong turn). 

\section{Baseline Mode}
As a baseline model for segment time prediction, the median travel time was calculated for the each segment over all journeys. To make predictions at a given point in the segment, the time travelled so far was subtracted from the median travel time and the result was used as the predicted remaining travel time.

\section{Gaussian Processes Regression for Trajectory Data}
This section presents how trajectory based Gaussian process regression
is used to predict arrival times. The core concept of the model is that each
trajectory $x$ and corresponding arrival time vector $y$ is
considered a realisation of a Gaussian process (\textit{GP}) modeling $y =
f(x)$. A GP generalises a multivariate
normal distribution, and is a distribution over
functions~\cite{Rasmussen-Williams-2006}, completely defined by its
mean function $m(x)$ and covariance function $k(x, x')$. For any input
vector $x$ to these functions the output $y$ is jointly normally distributed according to
\begin{equation}
  \label{eq:gp}
  y = f(x) \sim \mathcal{N}(\mu(x), \Sigma(x))
\end{equation}
where
\begin{equation}
  \label{eq:gp-mean-function}
  \mu(x) = m(x) + K(x, \textbf{x})\textbf{V}^{-1}{(y-m(x))}^{T}
\end{equation}
\begin{equation}
  \label{eq:gp-covariance-function}
  \Sigma(x) = K(x, x) + \sigma^{2}_n\textbf{I} - \textbf{K}(x, \textbf{x})\textbf{V}^{-1}{\textbf{K}(x, \textbf{x})}^{T},
\end{equation}
and $\textbf{K}$ is the gram matrix with elements $K_{ij} = k(x_i, x_j)$ and $\textbf{V}
= K(x, x) + \sigma_n^2I$. All models considered used the squared
exponential kernel
\begin{equation}
  k(x, x') = \sigma^2\exp(-\frac{(x-x')^{2}}{2l^2}),
\end{equation}
with parameters $\sigma$ and $l$ estimated by maximum aposteriori
estimation. In addition the measurement noise $\sigma_n$ was estimated
from data.

The model made predictions by computing the
most similar previously observed trajectories, and
based its prediction of their arrival times.
However, one big issue with this approach is that
trajectories are not directly comparable, since
the way vehicles travel can vary widely. This issue
was addressed by synchronising the trajectories using a
function estimated with a separate GP, which made them comparable,
before making the actual predictions. Following sections will present
in more detail the two steps of synchronisation and prediction.

\subsection{Synchronisation}
As previously mentioned, there is no natural way to compare
trajectories spatially. This is because the buses do not drive
\textit{precisely} the same route every time, for reasons such as
traffic, weather conditions and road work. To be able to train a model
based on comparing trajectories, the trajectories first had to be
mapped onto a synchronised space $\tau = [0, 1]$, where they could be
compared meaningfully. In this space, consecutive data points would
have a $\tau$ representing their normalised progression along a
trajectory. They would consequently be monotonically increasing in
$\tau$, starting at $\tau=0$ and ending at $\tau=1$. This was done by
training a GP modeling a synchronising function $f^{(l,s)}_s(x, y) : \mathcal{R}^2 \mapsto
\tau$ for each bus line $l$ and segment $s$ on a hand-picked
trajectory. There were two hurdles with this though. Firstly, the data
contained many subsegments with buses standing still, which unevenly
distributed the data spatially. This made the model prioritise getting
said subsegments right, while ignoring areas where data was more
sparse. Secondly, a GP model does not guarantee that
it models a smooth mapping with respect to progression, which was a
critical property for this application. To solve the issue of buses
standing still, a technique we call \textit{stop-compression} was
used, and to force the GP to model a smooth mapping with respect to
progression, \textit{support-data} was added. These two issues and the techniques used to solve
them are described in more detail Section~\ref{sec:stop-compression}
and Section~\ref{sec:support-data}. The
hyperparameters all had Gamma priors such that $E[\sigma] = 0.5$,
$Var(\sigma) = 0.04$, $E[l] = 1.6$, $Var(l) = 0.1$,
$E[\sigma_n]= 0.005$, $Var(\sigma_n) = 10^{(-6)}$.

\subsubsection{Stop-Compression}
\label{sec:stop-compression}
Since all buses produced one data point every second, a lot of
clustered data points were generated while a bus was standing still on
a parking lot before a journey, or when encountering a
red-light. However, the GPs that was used had a static length scale, which came with the assumption that the data was uniformly distributed.

\begin{figure}[H]
  \begin{minipage}{.46\textwidth}
    \includegraphics[scale=0.48,width=\textwidth]{figures/traj-without-stop-compression3.png}
    \caption{Estimated trajectory progression for $f_s^{(l, s)}$ on test trajectories without stop-compression.}
    \label{fig:progression-without-stop-compression}
  \end{minipage}
  \hspace{5pt}
  \begin{minipage}{.46\textwidth}
    \includegraphics[scale=0.5,width=\textwidth]{figures/traj-with-stop-compression3.png}
    \caption{Estimated trajectory progression for $f_s^{(l, s)}$ on test trajectories with stop-compression.}
    \label{fig:progression-with-stop-compression}
  \end{minipage}
\end{figure}

\noindent
This assumption did not hold with data points clustered on the same
spot. To get around that, all the data points generated during a
stand-still were clustered into one with the mean values of all
clustered points. As shown in
Figure~\ref{fig:progression-without-stop-compression} and
Figure~\ref{fig:progression-with-stop-compression}, the estimated
progression much more closely resemble the actual progression when stop-compression is used, indicated by the linear relationship.

\subsubsection{Support-Data}
\label{sec:support-data}
In addition to requiring the data to be uniformly distributed, it was essential that $f^{(r,s)}_s$ was a smooth mapping with respect to the direction of spatial progress. That is, data points close in spatial progression should be mapped onto points in $\tau$ which are also close. Furthermore, data distributed orthogonally to the direction of spatial progression should be mapped onto the same point in $\tau$. This requirement is quite reasonable when one considers that a bus driving slightly more to the left on a road is no closer to its destination than a bus driving slightly more to the right. 

\begin{figure}[H]
  \begin{minipage}{.46\textwidth}
    \includegraphics[scale=0.48,width=\textwidth]{figures/traj-without-support-data2.png}
    \caption{Spatial progression of a trajectory without support-data.}
    \label{fig:traj-without-support-data}
  \end{minipage}
  \hspace{5pt}
  \begin{minipage}{.46\textwidth}
    \includegraphics[scale=0.5,width=\textwidth]{figures/traj-with-support-data2.png}
    \caption{Spatial progression of a trajectory with support-data.}
    \label{fig:traj-with-support-data}
  \end{minipage}
\end{figure}

\noindent
In order to force the GP to model a function with this property, each
data point $d^{(i)}$ was duplicated by placing a normal distribution
over it, orthogonal to the spatial progression vector ${(d^{(i+1)}_x -
  d^{(i)}_x, d^{(i+1)}_y - d^{(i)}_y)}^T$, and drawing several samples
with the same progression as $d^{(i)}$. This process is illustrated in
Figure~\ref{fig:traj-without-support-data} and
Figure~\ref{fig:traj-with-support-data}. The generated support-data
was saved to a separate dataset and combined with the trajectory data
when training synchronisation GPs.

\begin{figure}[H]
  \begin{minipage}{.46\textwidth}
    \includegraphics[scale=0.48,width=\textwidth]{figures/heat-without-support-data.png}
    \caption{Contour plot of the posterior mean function of a
      synchronisation GP without using support-data.}
    \label{fig:heightmap-without-support}
  \end{minipage}
  \hspace{5pt}
  \begin{minipage}{.46\textwidth}
    \includegraphics[scale=0.5,width=\textwidth]{figures/heat-with-support-data.png}
    \caption{Contour plot of the posterior mean function of a
      synchronisation GP using support-data.}
    \label{fig:heightmap-with-support}
  \end{minipage}
\end{figure}

\noindent
As shown in Figure~\ref{fig:heightmap-without-support} and
Figure~\ref{fig:heightmap-with-support}, using support data greatly
increase how orthogonal the contours of $f^{(l,s)}_s$ are to the
spatial progression, which indicate the desired properties.

\subsection{Prediction}
After the synchronisation GPs were trained, it was possible to train
the model for making the actual predictions. Training was done by
first mapping all the training trajectories onto $\tau$ using the set
of learned $f^{(l,s)}_s$, and then training two GPs on the
synchronised trajectory. The first one was the inverse of the
synchronisation GP, ${GP_l^{(t, l, s)}}$, modeling
${f^{(t, l, s)}_s}^{-1}(\tau) : \tau \mapsto \mathcal{R}^2$,
and the second
was the actual prediction GP, $GP_p^{(t,l,s)}$, which modeled
${f^{(t,l,s)}_p}(\tau) : \tau \mapsto \mathcal{R}$. The purpose
of the first one was to compute log likelihoods of new trajectories
(hence the subscript $l$), and
the purpose of the second one was to make actual predictions of
arrival time for specific trajectories. Both $GP_l$ and $GP_s$ were stored to
disk, to be loaded as needed when presented with a new trajectory on
the corresponding segment $(l, s)$. Both depend on trajectory
$t$, bus line $l$ and segment $s$ but for more compact notation these
superscripts are dropped for the rest of this section.

The log likelihoods of $GP_l$ for a new trajectory $\bar{x}$,
synchronised onto $\bar{\tau}$ was given by
\begin{equation}
  \label{eq:model-log-likelihood}
  \begin{split}
    \log P(\bar{\tau}|\bar{x}, {GP_l}) & = -\frac{1}{2}(\bar{\tau} - \mu(\bar{x})){[\Sigma]}^{-1}(\bar{x} - \mu(\bar{\tau})) \\
    & = -\frac{1}{2}\log{|\Sigma|}+C,
  \end{split}
\end{equation}
where $\mu(\bar{x})$ and $\Sigma(\bar{x})$ are given by
equations~\ref{eq:gp-mean-function},
and~\ref{eq:gp-covariance-function} for a specific ${GP_l}$. Applying Bayes
theorem gave the unnormalised log distribution
\begin{equation}
  \label{eq:model-log-model-probability}
  \log P(GP_l | \bar{\tau}, \bar{x}) \propto \log P(\bar{\tau}|\bar{x}, GP_l) + \log P(GP_l)
\end{equation}
over models, where a uniform prior was assumed. Since
${GP_l}$ and $GP_p$ were trained on the same $l$ and $t$, 
$\log P(GP_p | \bar{\tau}, \bar{x})$ was also given by
equation~\ref{eq:model-log-model-probability}, which was then used to make 
arrival time predictions $\bar{t}$ by taking the mean of the mixture
model given by
\begin{equation}
  \label{eq:model-mixture}
  P(\bar{t}|\bar{x}) = \sum_{i=0}^{N}{P(\bar{t}|GP_p)P(GP_p | \bar{\tau},\bar{x})},
\end{equation}
which is illustrated in Figure~\ref{fig:mixture-start-of-traj} and
Figure~\ref{fig:mixture-end-of-traj}. The figures also show how the models certainty
increases as a trajectory progresses.
\begin{figure}[H]
  \begin{minipage}{.46\textwidth}
    \includegraphics[width=\textwidth]{figures/mixture-start-of-traj.png}
    \caption{Density of arrival times at the start of a segment.}
    \label{fig:mixture-start-of-traj}
  \end{minipage}
  \hspace{5pt}
  \begin{minipage}{.46\textwidth}
    \includegraphics[width=\textwidth]{figures/mixture-end-of-traj.png}
    \caption{Density of arrival times at the end of a segment.}
    \label{fig:mixture-end-of-traj}
  \end{minipage}
\end{figure}
\noindent
The hyperparameters for both $GP_l$ and $GP_s$ all had Gamma
priors. For $GP_l$ they were chosen so that $E[\sigma] = 2$,
$Var(\sigma) = 1$, $E[l] = 0.8$, $Var(l) = 0.2$, $E[\sigma_n] = 0.1$,
$Var(\sigma_n) = 0.005$. For $GP_s$ they were chosen so that $E[\sigma] = 2$,
$Var(\sigma) = 1$, $E[l] = 0.8$, $Var(l) = 0.2$, with a static
$\sigma_n = 0.05$. 

\section{Artificial Neural Networks}

Artificial neural networks have shown to be useful when predicting travel times due to their ability to model nonlinear relationships between features \cite{brazilANN, malaysiaANN}. This section introduces some concepts and parameters that were considered when creating artificial neural networks for this report, followed by an explanation of how the models were created and evaluated.

\subsection{Activation Functions}
The purpose of the activation function is to compute the hidden layer values \cite{Goodfellow-et-al-2016}. Some popular activation functions are the sigmoid function:

\begin{equation} 
f(x) = \frac{1}{1+e^{-x}},
\end{equation}

the rectifier function:

\begin{equation} 
f(x) = max\{0,x\}
\end{equation}

and \textit{tanh}:
\begin{equation} 
f(x) =  \frac{e^x-1}{e^{x}+1} 
\end{equation}

All three have been considered for the models in this report but only the rectifier function and \textit{tanh} are used in the models. They have been chosen through parameter optimisation and problem specific choices. For example, the output neuron of each model uses the rectifier function, since the prediction should be a positive value.

\subsection{Recurrent Neural Networks}
Recurrent neural networks (RNN) are a type of neural networks that can be used to make predictions based on a time-series \cite{RNN}. Instead of treating each data point independently like in a regular artificial neural network, an RNN handles sequences of subsequent data points making it possible to take previously observed data points into account. They have been used to predict bus travel times, and the best results were achieved using Long short-term memory (LSTM) nodes \cite{RNNBusPredictions}. One of the models that are implemented in this report is an RNN. 


\subsection{Implementation}
A number of different neural network models were implemented and tested. The models were created using \textit{Keras}\footnote{https://keras.io/} on top of \textit{Tensorflow}\footnote{https://www.tensorflow.org/}. The created models were both based on suggestions by papers \cite{brazilANN,RNNBusPredictions}, tailored to the Östgötatrafiken dataset. All models will not be mentioned in this report, only the most significant ones.

\subsection{Additional Pre-processing}
Some additional pre-processing was required to train the ANN models. Most importantly the data had to be labelled with the correct labels used to train the models. Datasets acquired from the pre-processing described in section \ref{pre-processing} were complemented with the columns

\begin{itemize}
	\item The time it takes to travel the entire current segment (seconds) - used as true label in Model 1
	\item The time left until next bus stop (seconds) - used as true label in Models 2,3,4
	\item The time from the start of the journey to the start of the current segment (seconds) - used in all models
	\item History with the three latest GPS coordinates and speed entries.
\end{itemize}

\noindent The first two columns that are added are used as true labels in the respective models. The third one is a feature which captures conditions earlier in the journey. The last column is used to let the network capture traffic conditions.

\subsection{Model Creation}
Each model was implemented separately. They take the data from the previous step and outputs a new data frame with 

\begin{itemize}
	\item Segment number
	\item Journey number
	\item Speed
	\item Prediction
	\item Label
\end{itemize}

Predictions are calculated once per observation in order to create easily comparable models.

\subsection{Model M1}
Model M1 was created based on the structure described by Fan and Gurmu \cite{brazilANN}. The model predicts the time it takes to travel the current segment. It is assumed that the time is known when the bus left the latest station. Then, the time left to the next station is calculated by subtracting the time that has passed from the current station from the segment travel time prediction.

As it turned out, model M1 produced the same segment prediction for every data point, making it unable to correct potential errors. This is seen in Figure \ref{fig:ann-m1}.


\begin{figure}[h]
	\begin{minipage}{\textwidth}
		\begin{adjustbox}{center}
			\includegraphics[width=1.3\textwidth]{ann-m1}
		\end{adjustbox}  
		\caption{One example journey for Bus 3 with predictions from model M1. Each point in time describes the time left to the next bus stop, hence the sawtooth look. The color of predictions represent normalized speed.}
		\label{fig:ann-m1}      
	\end{minipage}
\end{figure}


\subsection{Model M2}\label{M2}
With model M2, the idea was to let the model itself predict the time left to the next segment, in contrast to model M1. Also, more features were added. A complete list of features can be found in Table \ref{tbl:nn-features}. In contrast to model M1, as depicted in Figure \ref{fig:ann-m2}, this model is able to improve the predictions as time progresses. When evaluating this model, another problem was discovered which was the dwell time, where the model has trouble predicting when the bus is standing still. 


\begin{figure}[h]
	\begin{minipage}{\textwidth}
		\begin{adjustbox}{center}
			\includegraphics[width=1.3\textwidth]{ann-m2}
		\end{adjustbox}  
		\caption{One complete journey for Bus 3 with predictions from model M2. Each point in time describes the time left to the next bus stop, hence the sawtooth look. The color of predictions represent normalized speed.}
		\label{fig:ann-m2}      
	\end{minipage}
\end{figure}



\subsection{Model M3}\label{M3}
Model M3 model is very similar to model M2 with the addition of history from the three latest observations of position and speed data. The idea with historical data is that it may help cope with dwell time.

\subsection{Model M4}
Model M4 takes a different approach than model M3 in the way it takes historical observations into account. It is a recurrent neural network taking a sequence of 20 consecutive data points as input. It predicts the time left to the next bus stop like models M2 and M3. 

\subsection{Stop-compressed models}
As the models still had some trouble dealing with dwell time, models M1 through M4 were also evaluated with data that was stop-compressed (as described in section \ref{sec:stop-compression}) before calculating MAE and MAPE. 

\subsection{Layers and Features}
Table \ref{tbl:nn-layers} presents the layer types and activation functions that were used during model creation.

\begin{table}[h]
	\centering
	\caption{ANN layers used for different models. FC = \textit{Fully Connected}.}
	\label{tbl:nn-layers}
	\begin{tabular}{l|l|l|l|l}
		& \multicolumn{4}{c}{Models}                                                                                           \\ \cline{2-5} 
		& \multicolumn{2}{c|}{M1, M2, M3}                           & \multicolumn{2}{c}{M4}                                   \\ \hline
		Layer & Type                     & Activation                     & Type                     & Activation                     \\ \hline
		1     & FC, 2 * \#features nodes & RelU                           & LSTM, 128 nodes          & Tanh                        \\
		2     & FC, 1 * \#features nodes & Tanh                           & LSTM, 128 nodes          & Tanh                        \\
		3     & FC, 1 * \#features nodes & \textit{None} & LSTM, 128 nodes          & Tanh                        \\
		4     & output, 1 node           & RelU                           & FC, 32 nodes & RelU                           \\
		5     &                          &                                & output, 1 node           & RelU
	\end{tabular}
\end{table}

Table \ref{tbl:nn-features} describe the features used in the different models. Note that model M4 does not have the two last features (7 and 8), instead it captures this with a sequence of 20 data points with all other features.

\begin{table}[h]
	\caption{Features used in the different ANN models}
	\label{tbl:nn-features}
	\begin{adjustbox}{center}
		\begin{tabular}{l|llcc}
			& \multicolumn{4}{c}{Model}                                                                                                                                                                                                                                                                                                                                                     \\ \hline
			Feature & \multicolumn{1}{l|}{M1}                                                                                                               & \multicolumn{1}{l|}{M2}                                                                           & \multicolumn{1}{l|}{M3}                                                                                  & \multicolumn{1}{l}{M4} \\ \hline
			1       & \multicolumn{4}{c|}{\begin{tabular}[c]{@{}l@{}}Segment number (1-hot-encoded)\end{tabular}} \\ \hline
			2       & \multicolumn{4}{c|}{Time of day (hour)}\\ \hline
			3       & \multicolumn{4}{c|}{\begin{tabular}[c]{@{}l@{}}Time travelled until the start of the current segment (normalised)\end{tabular}} \\ \hline
			4       & \multicolumn{1}{l|}{-}                                                                                                                 & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}l@{}}Bus direction (degrees, cyclical)\end{tabular}}\\ \hline
			5       & \multicolumn{1}{l|}{-}                                                                                                                 & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}l@{}}Bus speed (m/s, normalized)\end{tabular}}\\ \hline
			6       & \multicolumn{1}{l|}{-}                                                                                                                 & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}l@{}}Bus GPS position (normalized)\end{tabular}}\\ \hline
			7       & \multicolumn{1}{l|}{-}                                                                                                                 & \multicolumn{1}{l|}{-}                                                                             & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}l@{}}Bus GPS position (history, 3 time steps)\end{tabular}}  & \multicolumn{1}{l|}{-}                                                                             \\ \hline
			8       & \multicolumn{1}{l|}{-}                                                                                                                 & \multicolumn{1}{l|}{-}                                                                             & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}l@{}}Bus speed (history, 3 time steps)\end{tabular}} & \multicolumn{1}{l|}{-}                                                                            
		\end{tabular}
		
	\end{adjustbox}
\end{table}

\subsection{Internal evaluation}
As the last step, the models were compared with the MAE and are presented one random journey at a time. The loss in the model was also compared with the data without the first segment, as this segment included a lot of dwell time, heavily impacting the performance. Finally, as some papers have shown good results \cite{kalmanPrediction, brazilANN}, a Kalman Filter was applied on top of the predictions. However, this was only done in the internal evaluation as the results were not improved by the kalman filter.

\section{Model evaluation}
To evaluate the performance of the models, four predictions per segment were made for each journey. All predictions estimate the remaining travel time until the end of the segment (until the next bus stop). There is one prediction each being made once the bus has travelled 20\%, 40\%, 60\%, and 80\% of the total segment. For instance, at the 20\% prediction, the model has access to all observations in the first 20\% of the segment. The models used an 80/20 split of the data into train/test sets, using the metrics MAE and MAPE to evaluate performance.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% lorem.tex ends here
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "demothesis"
%%% End: 
