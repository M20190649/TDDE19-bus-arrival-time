%%% lorem.tex --- 
%% 
%% Filename: lorem.tex
%% Description: 
%% Author: Ola Leifler
%% Maintainer: 
%% Created: Wed Nov 10 09:59:23 2010 (CET)
%% Version: $Id$
%% Version: 
%% Last-Updated: Wed Nov 10 09:59:47 2010 (CET)
%%           By: Ola Leifler
%%     Update #: 2
%% URL: 
%% Keywords: 
%% Compatibility: 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Commentary: 
%% 
%% 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Change log:
%% 
%% 
%% RCS $Log$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Code:

\chapter{Discussion}
\label{cha:discussion}

This chapter start out with a discussion of the results of the project, followed by discussions of the chosen methods. Lastly there are some suggestions for future work.

\section{Results}
\label{sec:discussion-results} 
There are differences in the datasets due to the division of the 20\%, 40\%, 60\% and 80\% sections being done after stop-compression. This results in that the 20\% division in a stop-compressed trajectory is further into the journey than in a trajectory that is not stop-compressed. If the data was split on timestamp that would not differ, but, since the data is split by index, it does. This, most likely, has an effect on the results and therefore models trained on stop-compressed data cannot really be compared to models trained on data that was not stop-compressed in a meaningful way.

As can be seen in table \ref{tbl:models-mae-and-mape-203}, the neural networks performed best in terms of both MAE and MAPE, on both the regular data and the stop-compressed data. The baseline method and M1 is on par, and their MAPE blows up when predicting on smaller parts of the segment. For the 80\% prediction, the predicted time left is larger than the actual time left, yielding an MAPE above 100\%. The GPR however performs quite poorly in comparison to the neural network models.

When considering the evaluation segment-wise, see table \ref{tbl:model-mae-of-segs-203}, the neural network approaches also dominates, the models trained on stop-compressed data yields the best results in terms of MAE, specially M2 and M3. Table \ref{fig:model-mape-of-segs-203} shows that in relative terms, the models trained on non-compressed data sometimes yields better results. This is probably due to the differences in the trajectory splits described previously. However, M2 and M3 still outperforms the other models, with exception of M4 which excels on non-compressed data.

In terms of complexity, the neural networks require much less resources than the implemented GPR model in order to make a prediction with the current implementation, and the GPR will become even more expensive given more trajectories to compare with. The GPR would however, most likely, improve its performance given more trajectories. There are methods, such as using inducing points that could improve GPR complexity consumption.

The only models that was evaluated for bus line 11 was the baseline method and M4. M4 performs better than the baseline on all metrics and all segments. 

\section{Method}
\label{sec:discussion-method}
\subsection{Evaluation Methods}
There a two main differences in the methods that we have used. The baseline method and the M1 model make predictions on complete segments. These two methods do not have the capability of making sophisticated predictions within a segment; therefore intra-segment predictions are made by subtracting the time spent on the segment from the time predicted for the whole segment. This leads to an increased MAPE as the prediction error remains constant, but with using less and less distance to predict on, the constant error corresponds to a larger part of the sub-segment.

\subsection{Neural Networks}
The neural networks that were trained and evaluated on stop-compressed data performed much better overall with regards to both MAE and MAPE. This was expected since the stop-compression removes most of the data points observed during dwell time when the bus is standing still which is the main source of error for the models using non-compressed data. Models M2,M3 and M4 all have similar performance overall whereas the performance of M1 is on par with the baseline model. The reason for the poor performance of M1 is that, as can be seen in figure \ref{fig:ann-m1}, the prediction error of model M1 will be static over each segment due to the model predicting travel times for entire segments rather than time left until next bus stop like the other models.

For the models using non-compressed data, M4 performed slightly better than M3 and M2. A likely reason is that the M4 managed to model the dwell time better than the other models, although dwell time is still the main cause of error for this model. For the models using stop-compressed data, M2 and M3 had similar results and both outperformed M4. Due to hardware limitations, the longest sequence that could be used as input for M4 was 20 data points. It would be interesting to know if longer sequences would yield better results.

The neural networks seem to be able to generalize to other bus lines fairly well without having to change network parameters. Comparing the MAPE for bus line 3 and 11 in tables \ref{tbl:models-mae-and-mape-203} and \ref{tbl:models-mae-and-mape-211} it can be seen that it is a bit higher for bus 11, although this was expected since that bus travels through areas with heavy traffic which would likely be hard for the model to capture.

\subsection{Kalman filtering}
A Kalman filter was applied to all NN models but without improving the performance. This was due to the models constantly overpredicting or underpredicting the time left, meaning that the predictions were not distributed around the true value. This left the Kalman filter unable to correct the error since the implementation was of the most basic type, only able to handle gaussian noise. 

There are models designed for non-gaussian noise that could be implemented for future work.

\subsection{Gaussian Process Regression}
The Gaussian Process regression approach only used 200 trajectories to train on since it was computationally expensive to compare data with each trajectory. When using all trajectories, training and prediction took too long to run. Using fewer comparison trajectories means that the model considers fewer kinds of behaviours which makes it worse than a model considering all comparison trajectories.

Apart from the fact that not all training data was used, the models performance was worse than expected. At times it was even as bad as the baseline model. Furthermore, it would be expected that the MAE decrease as larger parts of trajectories are observed by the model, since it can find likely trajectories easier with more data points, but this was not the case. This implies a bug in the implementation or, more likely, that the trajectory that the synchronisation GP was trained on was not well chosen. The synchronisation GP is by far the most critical and the most difficult piece of the model, and if the trajectories is not synchronised properly, the likelihood computed to weigh predictions would be nonsensical. 

\section{Future Work}
\label{sec:future-work}
This section describes areas of improvements worth investigating for the different models.

\subsection{Neural Network}
Having the schedule of the buses available would allow for some interesting new approaches. At any given station the current delay could be fed into the neural network. Since bus drivers drives faster and dwells less when they are late, having this as an input could improve prediction accuracy. Furthermore, a very simple baseline model could be created using the schedule. By simply using the offset of the last bus it should be possible to make acceptable predictions, as two subsequent bus often have similar delays due to traffic conditions.

The recurrent neural network model (M4) could possibly perform better if a longer sequence of data points were used. Due to hardware constraints, the model in this work uses sequences of 20 data points as input, more than that required additional memory. Doing this would either require more additional hardware resources or some exploration of how to split the training up into multiple sessions where the data is divided into smaller chunks.

A different approach to making predictions with neural networks could be to output distributions instead of single point estimates. Such an approach would yield information about the uncertainty of the results.

\subsection{Gaussian Process Regression}
The most central part of the model is the synchronisation of trajectories, which is consequently the most important one to get right. For this implementation, the trajectory to train the synchronisation GP on was hand picked by manually trying different ones and observing plots. The trajectory used to train the synchronisation GP made a big difference on how good the GP mapped coordinates into progress, and it is very likely that the one picked is not the best since no methodical approach was taken. Trying something methodical for doing this would be a great improvement, and something as simple as taking the trajectory with mean arrival time would be something to start with. In order to make the synchronisation GP better support data was generated in each observed data point, but it could very well be done on a more fine-grained interval, which may improve its performance further.

When the model makes predictions it weighs previously observed trajectories using their likelihood. It is entirely possible that the likelihood can be weighted with a parameter learned from data to achieve a better performance in the final predictions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% lorem.tex ends here

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "demothesis"
%%% End: 
