{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrival Time Prediction - Performance\n",
    "This notebook is the final notebook on the work on GP regression for arrival time prediction, which measures the models performance. The data used is loaded from train/test pickle files which contains un-processed data. Since we assume that the synchronisation functions is constant, they are simply loaded from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 trajectories\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gp_gpy as gp\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def rename(d):\n",
    "    return d.rename(columns = {\n",
    "        'latitude': 'lat', \n",
    "        'longitude': 'lon', \n",
    "        'journey_number': 'traj', \n",
    "        'segment_number': 'seg', \n",
    "        'speed': 'speed',\n",
    "        'event': 'event',\n",
    "        'timestamp': 'timestamp'\n",
    "    })\n",
    "    \n",
    "route_n = 3\n",
    "train = rename(pd.read_pickle('./203_train.p')).sort_values(['traj', 'seg', 'timestamp'])\n",
    "test = rename(pd.read_pickle('./203_test.p')).sort_values(['traj', 'seg', 'timestamp'])\n",
    "\n",
    "n_train_trajs = train.traj.unique()\n",
    "n_train_trajs_to_use = 200\n",
    "trajs_to_use = frozenset(np.random.choice(n_train_trajs, size=n_train_trajs_to_use, replace=False))\n",
    "train = train.query('traj in @trajs_to_use')\n",
    "print('Loaded', train.traj.unique().shape[0], 'trajectories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stop_compress(data):\n",
    "    from math import radians, cos, sin, asin, sqrt, isnan\n",
    "    import dateutil.parser\n",
    "    \n",
    "    def haversine(lon1, lat1, lon2, lat2):\n",
    "        \"\"\"\n",
    "       Calculate the great circle distance between two points\n",
    "       on the earth (specified in decimal degrees)\n",
    "       \"\"\"\n",
    "        # convert decimal degrees to radians\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    " \n",
    "        # haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        return c * r\n",
    " \n",
    "    def as_dict(d, compressed):\n",
    "        return {\n",
    "            'lat': d.lat,\n",
    "            'lon': d.lon,\n",
    "            'seg': d.seg,\n",
    "            'speed': d.speed,\n",
    "            'traj': int(d.traj),\n",
    "            'timestamp': d.timestamp,\n",
    "            'event': d.event,\n",
    "            'compressed': compressed\n",
    "        }\n",
    "\n",
    "    # still requires double pd.to_datetime(..)\n",
    "    def mean_timestamp(timestamps):\n",
    "        x = timestamps.apply(dateutil.parser.parse)\n",
    "        epochs = [ t.timestamp() for t in x ]\n",
    "        mean_epoch = int(np.mean(epochs))\n",
    "        dt = pd.to_datetime(mean_epoch, unit = 's')    \n",
    "        return dt\n",
    " \n",
    "    def compress(data):\n",
    "        dict_data = [as_dict(x, 0) for x in data]\n",
    "        if len(dict_data) == 1: \n",
    "            return dict_data[0]\n",
    "   \n",
    "        df = pd.DataFrame(dict_data)   \n",
    "        df.speed = np.max(df.speed, 0) # data contains -1 sentinel values for missing speed\n",
    " \n",
    "        df2 = df.drop(['timestamp', 'event', 'seg'], axis=1).apply(np.mean, axis=0)\n",
    "        df2['timestamp'] = mean_timestamp(df['timestamp'])\n",
    "        df2['timestamp'] = pd.to_datetime(df2['timestamp'])\n",
    "   \n",
    "        contains_entered_event = lambda df : df.event.transform(lambda e : e == 'EnteredEvent').any()\n",
    "        df2['event'] = 'EnteredEvent' if contains_entered_event(df) else 'ObservedPositionEvent'\n",
    "        df2['seg'] = df.seg.min() # In the case of overlapping segments we let the data belong to the first\n",
    " \n",
    "        return as_dict(df2, 1)\n",
    "\n",
    "    delta = 4e-3 # approx. 4 metres\n",
    "    output = []\n",
    "    buffer = [ data.iloc[0] ]\n",
    "    for i, current in enumerate(data.itertuples()):\n",
    "        distance = haversine(\n",
    "            current.lat, \n",
    "            current.lon, \n",
    "            buffer[-1].lat, \n",
    "            buffer[-1].lon)\n",
    "   \n",
    "        if distance > delta:\n",
    "            output.append(compress(buffer))\n",
    "            buffer.clear()\n",
    "   \n",
    "        buffer.append(current)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            clear_output(wait=True)\n",
    "            display(str(datetime.datetime.now()) + ' processed row ' + str(i) + ' out of ' + str(data.shape[0]))\n",
    "\n",
    "\n",
    "    if len(buffer) > 0:\n",
    "        output.append(compress(buffer))\n",
    "\n",
    "    compressed_data = pd.DataFrame(output)\n",
    "    return compressed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-12 07:27:24.366484 processed row 191000 out of 191156'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train processed in 41.247472524642944 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "compressed_train = stop_compress(train)\n",
    "elapsed = time.time() - t0\n",
    "print(\"Train processed in\", elapsed, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-12 07:29:51.674837 processed row 693000 out of 693685'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test processed in 149.80568432807922 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "compressed_test = stop_compress(test)\n",
    "elapsed = time.time() - t0\n",
    "print(\"Test processed in\", elapsed, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def compute_progress(data):\n",
    "    time_left = pd.DataFrame({'time_left': np.zeros(data.shape[0])})\n",
    "    progress = pd.DataFrame({'progress': np.zeros(data.shape[0])})\n",
    "    reverse_rows = data.iloc[::-1]\n",
    "    last_stop_timestamp = reverse_rows.iloc[0].timestamp\n",
    "    trajs = data.traj.unique()\n",
    "    segs = data.seg.unique()\n",
    "\n",
    "    sdata = data.sort_values(['traj', 'seg', 'timestamp'])\n",
    "    cur_traj = 0\n",
    "    cur_seg = 0\n",
    "    tn = 0\n",
    "    seg0 = 0\n",
    "    segn = 0\n",
    "    traj = sdata\n",
    "    for i, d in enumerate(sdata.itertuples()):\n",
    "        if d.traj > cur_traj:\n",
    "            cur_seg = 0\n",
    "            cur_traj = d.traj\n",
    "            traj = sdata[sdata.traj == cur_traj]\n",
    "\n",
    "        if d.seg > cur_seg:\n",
    "            cur_seg = d.seg\n",
    "            seg0 = i\n",
    "            seg = traj[traj.seg == cur_seg]\n",
    "            segn = seg.shape[0]\n",
    "            tn = seg.iloc[-1].timestamp\n",
    "\n",
    "        time_left.iloc[i] = (tn - d.timestamp).seconds\n",
    "        progress.iloc[i]  = (i - seg0) / (segn - 1)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            clear_output(wait=True)\n",
    "            display(str(datetime.datetime.now()) + ' processed row ' + str(i) + ' out of ' + str(sdata.shape[0]))\n",
    "    \n",
    "    return pd.concat([\n",
    "        data[['lat', \n",
    "              'lon', \n",
    "              'traj', \n",
    "              'seg', \n",
    "              'speed']],\n",
    "        time_left,\n",
    "        progress],\n",
    "        axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-12 07:30:19.542147 processed row 109000 out of 109103'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train processed in 24.451486349105835  seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>traj</th>\n",
       "      <th>seg</th>\n",
       "      <th>speed</th>\n",
       "      <th>time_left</th>\n",
       "      <th>progress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.571011</td>\n",
       "      <td>58.414211</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.571172</td>\n",
       "      <td>58.414173</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.024390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.571271</td>\n",
       "      <td>58.414303</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2.93</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.048780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.570987</td>\n",
       "      <td>58.414417</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3.54</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.570906</td>\n",
       "      <td>58.414429</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.26</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.097561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat        lon  traj  seg  speed  time_left  progress\n",
       "0  15.571011  58.414211    11    1   0.00      136.0  0.000000\n",
       "1  15.571172  58.414173    11    1  -1.00       70.0  0.024390\n",
       "2  15.571271  58.414303    11    1   2.93       55.0  0.048780\n",
       "3  15.570987  58.414417    11    1   3.54       38.0  0.073171\n",
       "4  15.570906  58.414429    11    1   4.26       37.0  0.097561"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "progress_train = compute_progress(compressed_train)\n",
    "dt = time.time() - t0\n",
    "print(\"Train processed in\", dt, \" seconds\")\n",
    "progress_train.to_pickle('progress_train.pkl')\n",
    "progress_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-12 07:31:46.924308 processed row 391000 out of 391057'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test processed in 87.38213300704956  seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>traj</th>\n",
       "      <th>seg</th>\n",
       "      <th>speed</th>\n",
       "      <th>time_left</th>\n",
       "      <th>progress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.571018</td>\n",
       "      <td>58.414222</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.571172</td>\n",
       "      <td>58.414158</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.571032</td>\n",
       "      <td>58.414198</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2.88</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.571301</td>\n",
       "      <td>58.414256</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3.87</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.571110</td>\n",
       "      <td>58.414398</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4.61</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat        lon  traj  seg  speed  time_left  progress\n",
       "0  15.571018  58.414222    12    1   0.04      134.0  0.000000\n",
       "1  15.571172  58.414158    12    1  -1.00       66.0  0.023810\n",
       "2  15.571032  58.414198    12    1   2.88       64.0  0.047619\n",
       "3  15.571301  58.414256    12    1   3.87       55.0  0.071429\n",
       "4  15.571110  58.414398    12    1   4.61       46.0  0.095238"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "progress_test = compute_progress(compressed_test)\n",
    "dt = time.time() - t0\n",
    "print(\"Test processed in\", dt, \" seconds\")\n",
    "progress_test.to_pickle('progress_test.pkl')\n",
    "progress_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>traj</th>\n",
       "      <th>seg</th>\n",
       "      <th>speed</th>\n",
       "      <th>time_left</th>\n",
       "      <th>progress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.571011</td>\n",
       "      <td>58.414211</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.571172</td>\n",
       "      <td>58.414173</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.024390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.571271</td>\n",
       "      <td>58.414303</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2.93</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.048780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.570987</td>\n",
       "      <td>58.414417</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3.54</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.570906</td>\n",
       "      <td>58.414429</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.26</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.097561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat        lon  traj  seg  speed  time_left  progress\n",
       "0  15.571011  58.414211    11    1   0.00      136.0  0.000000\n",
       "1  15.571172  58.414173    11    1  -1.00       70.0  0.024390\n",
       "2  15.571271  58.414303    11    1   2.93       55.0  0.048780\n",
       "3  15.570987  58.414417    11    1   3.54       38.0  0.073171\n",
       "4  15.570906  58.414429    11    1   4.26       37.0  0.097561"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gp_gpy as gp\n",
    "from time import time\n",
    "from IPython.display import display, clear_output\n",
    "route_n = 3\n",
    "progress_train = pd.read_pickle('progress_train.pkl')\n",
    "progress_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "\n",
    "def synch_traj(synch_gp, data):\n",
    "    X = data[['lat', 'lon']].values\n",
    "    tau, _var = gp.predict(synch_gp, X)\n",
    "    return data.assign(tau=tau.reshape(tau.shape[0]))\n",
    "\n",
    "def train_seg_gps(seg, route_n, traj_n, seg_n, synch_gp):\n",
    "    synched_seg = synch_traj(synch_gp, seg)\n",
    "    n_restarts = 5\n",
    "     # Learn GP to compute likelihood of new data\n",
    "    likelihood_model = gp.build(\n",
    "        gp.LIKELIHOOD,\n",
    "        synched_seg[['tau']],\n",
    "        synched_seg[['lat', 'lon']],\n",
    "        route_n,\n",
    "        traj_n,\n",
    "        seg_n)\n",
    "    #likelihood_model.model.kern.lengthscale = 0.05\n",
    "    #likelihood_model.model.kern.variance = 5\n",
    "    likelihood_model.model.kern.lengthscale.set_prior(GPy.priors.Gamma.from_EV(0.8, 0.2))\n",
    "    likelihood_model.model.kern.variance.set_prior(GPy.priors.Gamma.from_EV(2, 1))\n",
    "    likelihood_model.model.likelihood.variance.set_prior(GPy.priors.Gamma.from_EV(0.1, 0.005))\n",
    "    #likelihood_model.model.likelihood.variance = 0.005\n",
    "    #likelihood_model.model.likelihood.variance.fix()\n",
    "    \n",
    "    #likelihood_model.model.likelihood.variance.set_prior(GPy.priors.Gamma.from_EV(0.001, 0.00005))\n",
    "    gp.train(likelihood_model, n_restarts)\n",
    "    \n",
    "    prediction_model = gp.build(\n",
    "        gp.PREDICTION, \n",
    "        synched_seg[['tau']], \n",
    "        synched_seg[['time_left']], \n",
    "        route_n, \n",
    "        traj_n, \n",
    "        seg_n)    \n",
    "    prediction_model.model.kern.lengthscale.set_prior(GPy.priors.Gamma.from_EV(0.8, 0.2))\n",
    "    prediction_model.model.kern.variance.set_prior(GPy.priors.Gamma.from_EV(2, 1))\n",
    "    prediction_model.model.likelihood.variance.set_prior(GPy.priors.Gamma.from_EV(0.1, 0.005))\n",
    "    prediction_model.model.likelihood.variance = 0.005\n",
    "    prediction_model.model.likelihood.variance.fix()\n",
    "    \n",
    "    # the variances are too different to capture with a good prior\n",
    "    #likelihood_model.model.likelihood.variance.set_prior(GPy.priors.Gamma.from_EV(0.001, 0.00005))\n",
    "    gp.train(prediction_model, n_restarts)\n",
    "    \n",
    "    return likelihood_model, prediction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gps(data, route_n, synch_gps, conn):\n",
    "    n_trajs = data.traj.unique()\n",
    "    trajs = dict(tuple(data.groupby('traj')))\n",
    "    for i, traj_n in enumerate(n_trajs):\n",
    "        traj = trajs[traj_n] \n",
    "        n_segs = traj.seg.unique()\n",
    "        segs = dict(tuple(traj.groupby('seg')))\n",
    "        for seg_n in n_segs:\n",
    "            seg = segs[seg_n]\n",
    "            synch_gp = synch_gps[seg_n]\n",
    "            lik_model, pred_model = train_seg_gps(seg, route_n, traj_n, seg_n, synch_gp)\n",
    "            gp.save(lik_model, conn)\n",
    "            gp.save(pred_model, conn)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(str(datetime.datetime.now()) + ': Trained ' + str(i) + ' out of ' + str(len(n_trajs)) + ' trajectories')\n",
    "\n",
    "        #name = LIKELIHOOD_MODEL + str(route_n) + '-' + str(traj_n) + '-' + str(seg_n)\n",
    "        #lik_model.model.save_model(name, compress=False)\n",
    "        #loaded_model = GPy.load(name)\n",
    "        #print(lik_model.model)\n",
    "        #print(gp.load(LIKELIHOOD_MODEL, route_n, traj_n, seg_n).model) \n",
    "        #print(pred_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-12 07:41:54.030795: Trained 199 out of 200 trajectories'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train processed in 606.3166592121124  seconds\n"
     ]
    }
   ],
   "source": [
    "#progress_train = progress_train[progress_train.seg > 1]\n",
    "synch_gps = {}\n",
    "with gp.acquire_db_conn() as conn:\n",
    "    for seg_n in progress_train.seg.unique():\n",
    "        synch_gps[seg_n] = gp.load_synch(route_n, seg_n, conn)\n",
    "\n",
    "t0 = time()\n",
    "route_n = 3\n",
    "\n",
    "train_gps(progress_train, route_n, synch_gps, conn)\n",
    "dt = time() - t0\n",
    "print(\"Train processed in\", dt, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "test = pd.read_pickle('progress_test.pkl')\n",
    "route_n = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded GPs for segment 11 out of 11'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Done!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gp_gpy as gp\n",
    "pred_gps = {}\n",
    "lik_gps = {}\n",
    "synch_gps = {}\n",
    "with gp.acquire_db_conn() as conn:\n",
    "    for i, seg_n in enumerate(test.seg.unique()):\n",
    "        synch_gps[seg_n] = gp.load_synch(route_n, seg_n, conn)\n",
    "        pred_gps[seg_n] = gp.load_trajs(gp.PREDICTION, route_n, seg_n, conn)\n",
    "        lik_gps[seg_n] = gp.load_trajs(gp.PREDICTION, route_n, seg_n, conn)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display('Loaded GPs for segment ' + str(i + 1) + ' out of ' + str(len(test.seg.unique())))\n",
    "display('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-13 23:12:59.470632'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Evaluting segment 11 out of 11'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Evaluated trajectory 720 out of 720'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import norm\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "def data_loglik(model, X, Y):\n",
    "    def loglik(x, y):\n",
    "        mu, sigma = gp.predict(model, x.reshape(1, 1))\n",
    "        return -0.5*(y-mu)*np.linalg.inv(sigma)*(y-mu).T \\\n",
    "                -0.5*np.log(np.abs(sigma))\n",
    "    return np.sum([loglik(x, y) for x, y in zip(X, Y)])\n",
    "\n",
    "def synch_traj(synch_gp, data):\n",
    "    X = data[['lat', 'lon']].values\n",
    "    tau, _var = gp.predict(synch_gp, X)\n",
    "    return data.assign(tau=tau.reshape(tau.shape[0]))\n",
    "\n",
    "seg_sizes = [.2, .4, .6, .8]\n",
    "mape = {s: defaultdict(dict) for s in seg_sizes}\n",
    "mae =  {s: defaultdict(dict) for s in seg_sizes}\n",
    "segs = dict(tuple(test.groupby('seg')))\n",
    "for seg_i, seg_n in enumerate(segs):\n",
    "    seg = segs[seg_n]    \n",
    "    trajs = dict(tuple(seg.groupby('traj')))\n",
    "    for traj_i, traj_n in enumerate(trajs):\n",
    "        whole_traj = trajs[traj_n]\n",
    "\n",
    "    \n",
    "        for size in seg_sizes:\n",
    "\n",
    "            # Extract just the first part of traj\n",
    "            n = round(whole_traj.shape[0]*size)\n",
    "            traj = whole_traj[:n]\n",
    "            # Synchronise it\n",
    "            synched_traj = synch_traj(synch_gps[seg_n], traj)\n",
    "            \n",
    "            # Compute weighted prediction models\n",
    "            X = synched_traj[['tau']].values\n",
    "            Y = synched_traj[['lat', 'lon']].values\n",
    "\n",
    "            logliks = [data_loglik(lg, X, Y) for lg in lik_gps[seg_n]]\n",
    "            models_with_loglik = zip(pred_gps[seg_n], logliks)\n",
    "\n",
    "            models_with_weights = [(m, np.exp(loglik - max(logliks))) for m, loglik in models_with_loglik]\n",
    "            preds_with_weights = \\\n",
    "                [gp.predict(m, X[-1,:].reshape(1, -1)) + (w,) \\\n",
    "                 for m, w in models_with_weights]\n",
    "\n",
    "            # Compute the mixture model\n",
    "            n = 8\n",
    "            delta = 1/n\n",
    "            sort = sorted(preds_with_weights, key=lambda mvw: mvw[0])\n",
    "            xmin = max(0, np.floor(float(sort[0][0]-sort[0][1]*3)))\n",
    "            xmax = np.ceil(float(sort[-1][0]+sort[-1][1]*3))\n",
    "            xx = np.linspace(xmin, xmax, (xmax-xmin)*n)\n",
    "            distributions = [w*norm.pdf(xx, float(mean), np.sqrt(float(var))) \\\n",
    "                             for mean, var, w in preds_with_weights]\n",
    "\n",
    "            mixture = reduce(np.add, distributions)\n",
    "            mixture = mixture/sum(mixture)\n",
    "\n",
    "            #plt.plot(xx, mixture)\n",
    "\n",
    "            # Make prediction\n",
    "            mean = xmin + delta*round(reduce(lambda tot, m: tot + m[0]*m[1], enumerate(mixture), 0))\n",
    "            #mode = xmin + delta*np.argmax(mixture)\n",
    "            y = traj.iloc[-1].time_left\n",
    "            pred = mean\n",
    "            mae[size][seg_n][traj_n]  = float(np.abs(y-pred))\n",
    "            mape[size][seg_n][traj_n] = float(np.abs((y-pred)/y))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(str(datetime.datetime.now()))\n",
    "        display('Evaluting segment ' + str(seg_i + 1) + ' out of ' + str(len(segs)))\n",
    "        display('Evaluated trajectory ' + str(traj_i + 1) + ' out of ' + str(len(trajs)))\n",
    "\n",
    "with open('mape.pkl', 'wb') as handle:\n",
    "    pickle.dump(mape, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('mae.pkl', 'wb') as handle:\n",
    "    pickle.dump(mae, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute score\n",
    "Percentage by perentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 0.2\n",
      "MAPE 0.2591408455675664\n",
      "MAE 12.177209595959596\n",
      "Size 0.4\n",
      "MAPE 0.34102098271775305\n",
      "MAE 10.723816287878789\n",
      "Size 0.6\n",
      "MAPE 0.6123383204883472\n",
      "MAE 10.751499368686869\n",
      "Size 0.8\n",
      "MAPE 1.5501106915380023\n",
      "MAE 13.416919191919192\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open('mape.pkl', 'rb') as handle:\n",
    "    mape = pickle.load(handle)\n",
    "\n",
    "with open('mae.pkl', 'rb') as handle:\n",
    "    mae = pickle.load(handle)\n",
    "    \n",
    "seg_sizes = [.2, .4, .6, .8]\n",
    "\n",
    "for size in seg_sizes:\n",
    "    final_mape = []\n",
    "    final_mae = []\n",
    "    for seg_n in segs:\n",
    "        for traj_n in segs[seg_n].traj.unique():\n",
    "            final_mape.append(mape[size][seg_n][traj_n])\n",
    "            final_mae.append(mae[size][seg_n][traj_n])\n",
    "    print('Size', size)\n",
    "    print('MAPE', np.mean(final_mape))\n",
    "    print('MAE', np.mean(final_mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-9c3c1de971bd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-9c3c1de971bd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Segment by segment\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Segment by segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg 1\n",
      "MAPE 0.8382465181145535\n",
      "MAE 12.107465277777777\n",
      "seg 2\n",
      "MAPE 0.9314885423978633\n",
      "MAE 11.660807291666666\n",
      "seg 3\n",
      "MAPE 0.49683974065651226\n",
      "MAE 6.899175347222222\n",
      "seg 4\n",
      "MAPE 1.6874164245156607\n",
      "MAE 15.997916666666667\n",
      "seg 5\n",
      "MAPE 0.3724442174142777\n",
      "MAE 6.131423611111111\n",
      "seg 6\n",
      "MAPE 0.3669121000466169\n",
      "MAE 12.19123263888889\n",
      "seg 7\n",
      "MAPE 0.5440671979150623\n",
      "MAE 13.235503472222222\n",
      "seg 8\n",
      "MAPE 1.3120066579870044\n",
      "MAE 14.459592013888889\n",
      "seg 9\n",
      "MAPE 0.2763091506032301\n",
      "MAE 22.112196180555557\n",
      "seg 10\n",
      "MAPE 0.3719260054822329\n",
      "MAE 6.884548611111111\n",
      "seg 11\n",
      "MAPE 0.39952325572407593\n",
      "MAE 7.761111111111111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = pd.read_pickle('progress_test.pkl')\n",
    "segs = dict(tuple(test.groupby('seg')))\n",
    "for seg_n in segs:\n",
    "    final_mape = []\n",
    "    final_mae = []\n",
    "    for size in seg_sizes:\n",
    "        for traj_n in segs[seg_n].traj.unique():\n",
    "            final_mape.append(mape[size][seg_n][traj_n])\n",
    "            final_mae.append(mae[size][seg_n][traj_n])\n",
    "    print('seg', seg_n)\n",
    "    print('MAPE', np.mean(final_mape))\n",
    "    print('MAE', np.mean(final_mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "def plot_likelihood_models(models_by_loglik, synched_traj):\n",
    "    \n",
    "    def to_df(m, i):\n",
    "        df = pd.DataFrame(gp.predict(m, X)[0], columns=['lat' ,'lon'])\n",
    "        df['Similarity rank'] = i\n",
    "        return df\n",
    "    \n",
    " \n",
    "    #print([(m[0].route_n, m[0].traj_n, m[0].seg_n) for m in models_by_loglik[:n_models_to_plot]])\n",
    "    X = synched_traj[['tau']].values\n",
    "    n_models_to_plot = 10\n",
    "    models_to_plot = models_by_loglik[:n_models_to_plot]\n",
    "    n_models = len(models_to_plot)\n",
    "    dfs = [to_df(mloglik[0], i) for mloglik, i in zip(models_to_plot, range(n_models))]\n",
    "    traj_df = synched_traj.copy()\n",
    "    traj_df['Similarity rank'] = 0\n",
    "    plt_df = pd.concat([traj_df] + dfs)\n",
    "    \n",
    "    sns.scatterplot(data=plt_df, x='lat', y='lon', hue='Similarity rank', ax=axs[0, 1])\n",
    "    axs[0, 1].set_title('New trajectory with the most similair known trajectories')\n",
    "    axs[0, 1].set_aspect('equal', 'datalim')    \n",
    "    center_axs(axs[0, 1], synched_traj, ['lat', 'lon'])\n",
    "\n",
    "    \n",
    "def center_axs(ax, data, XY):\n",
    "    ax.set_xlim(data[XY[0]].min()*0.99999, data[XY[0]].max()*1.00001)\n",
    "    ax.set_ylim(data[XY[1]].min()*0.99999, data[XY[1]].max()*1.00001)\n",
    "    \n",
    "def mogp(traj, synch_gp, route_n, seg_n, plot):\n",
    "    \n",
    "    def data_loglik(model, X, Y):\n",
    "        def loglik(x, y):\n",
    "            mu, sigma = gp.predict(model, x.reshape(1, 1))\n",
    "            return -0.5*(y-mu)*np.linalg.inv(sigma)*(y-mu).T \\\n",
    "                    -0.5*np.log(np.abs(sigma))\n",
    "        return np.sum([loglik(x, y) for x, y in zip(X, Y)])\n",
    "        \n",
    "    def make_prediction(model, data):\n",
    "        X = data[['lat', 'lon']].values.reshape(data.shape[0], 2)\n",
    "        return gp.predict(model, X)\n",
    "\n",
    "    def corresponding_pred_model(lik_model):\n",
    "        return gp.load(PREDICTION_MODEL, lik_model.route_n, lik_model.traj_n, lik_model.seg_n)\n",
    "        \n",
    "    def weighted_models(synched_traj, models):\n",
    "        \"\"\"\n",
    "        Returns models and their weights, weighted by the posterior predictive probability of the model\n",
    "        normalised over the maximum likelihood assuming uniform model prior. \n",
    "        So the most probable model has weight w = 1.0, and the less likely models has weights 0 < w < 1.0.\n",
    "        \"\"\"\n",
    "        X = synched_traj[['tau']].values\n",
    "        Y = synched_traj[['lat', 'lon']].values\n",
    "        models_with_loglik = [(m, data_loglik(m, X, Y)) for m in models]\n",
    "        models_by_loglik = sorted(models_with_loglik, key=lambda mw: mw[1], reverse=True)\n",
    "        max_loglik = models_by_loglik[0][1]\n",
    "        \n",
    "        if plot:\n",
    "            plot_likelihood_models(models_by_loglik, synched_traj)\n",
    "            \n",
    "        return [(m, np.exp(loglik - max_loglik)) for m, loglik in models_by_loglik]\n",
    "    \n",
    "    synched_traj = synch_traj(synch_gp, traj)\n",
    "    lik_models = gp.load_trajs(LIKELIHOOD_MODEL, route_n, seg_n)\n",
    "    lik_models_with_weights = weighted_models(synched_traj, lik_models)\n",
    "    pred_models_with_weights = [(corresponding_pred_model(lik_m), w) for lik_m, w in lik_models_with_weights]\n",
    "    #print('Most similair model', lik_models_with_weights[0][0].route_n, lik_models_with_weights[0][0].traj_n, lik_models_with_weights[0][0].seg_n)\n",
    "    \n",
    "    if plot:\n",
    "        pred_models_with_weights[0][0].model.plot(ax=axs[2, 1])\n",
    "        axs[2, 1].set_title('Most similair prediction GP')\n",
    "        \n",
    "    latest_tau = synched_traj.iloc[-1][['tau']].values.reshape(1, 1)\n",
    "    mean_var_weights = [gp.predict(m, latest_tau) + (w,) for m, w in pred_models_with_weights]\n",
    "    return mean_var_weights\n",
    "\n",
    "def predict(traj, synch_gp, route_n, seg_n, arrival_time=None, plot=False):\n",
    "    mean_var_weights = mogp(traj, synch_gp, route_n, seg_n, plot)\n",
    "    mean_var_weights = mean_var_weights #[:10] \n",
    "    if plot:\n",
    "        sort = sorted(mean_var_weights, key=lambda mvw: mvw[0])\n",
    "        xmin = max(0, np.floor(float(sort[0][0]-sort[0][1]*3)))\n",
    "        xmax = np.ceil(float(sort[-1][0]+sort[-1][1]*3))\n",
    "        x = np.linspace(xmin, xmax, 200)\n",
    "        \n",
    "        if plot:\n",
    "            for mean, var, w in mean_var_weights:\n",
    "                axs[1,0].plot(x, w*norm.pdf(x, float(mean), np.sqrt(float(var))))\n",
    "\n",
    "            axs[1,0].set_title('Mixture of predictions')\n",
    "            axs[1,0].set_xlabel('Seconds')\n",
    "            axs[1,0].set_ylabel('Density')\n",
    "            if arrival_time:\n",
    "                axs[1,0].axvline(x=arrival_time, label='True arrival time')\n",
    "                axs[1,0].legend()\n",
    "\n",
    "    #models_by_similarity = sorted(mean_var_weights, key=lambda p: p[2], reverse=True)\n",
    "    #models_by_mode = sorted(mean_var_weights, key=lambda p: p[0]*p[2], reverse=True)\n",
    "    total_weight = sum([w for _, _, w in mean_var_weights])\n",
    "    weighted_mean = sum([w*float(mean) for mean, var, w in mean_var_weights])/total_weight\n",
    "    \n",
    "    #weighted_means = [w*float(mean) for mean, var, w in mean_var_weights]\n",
    "    #print(weighted_means)\n",
    "    #mode_model = models_by_mode[0] #mean_var_weights[np.argmax(weighted_means)]\n",
    "    \n",
    "    #print(mode_model)\n",
    "    #n = 200\n",
    "    #distribution = np.sum([norm.pdf(n, float(mean), np.sqrt(float(var))) for mean, var, w in mean_var_weights])\n",
    "   \n",
    "    #print(weighted_means)\n",
    "    #print(mode_model)\n",
    "    return weighted_mean #float(mode_model[0])\n",
    "\n",
    "#np.random.seed(8)\n",
    "plot = True\n",
    "if plot:\n",
    "    fig, axs = plt.subplots(3,2)\n",
    "    fig.set_figwidth(16)\n",
    "    fig.set_figheight(24)\n",
    "    \n",
    "test = train #pd.read_pickle('test.pkl')\n",
    "n_test_trajs_to_use = 38\n",
    "n_test_trajs = test.traj.unique()\n",
    "test = test[test.traj < n_test_trajs[n_test_trajs_to_use]]\n",
    "traj_n = n_test_trajs[0]\n",
    "test_traj = test[test.traj == traj_n]\n",
    "seg_ix = 3\n",
    "seg_n = test_traj.seg.unique()[seg_ix]\n",
    "test_seg = test_traj[test_traj.seg == seg_n]\n",
    "def predict_seg_with_plots(seg, route_n, seg_n, true_pred_gp=None):\n",
    "    i = test_seg.index[0]+5 #np.random.randint(test_seg.index[0], test_seg.index[-1])\n",
    "    seg = seg[seg.index <= i]\n",
    "    synch_gp = gp.load_synch(route_n, seg_n, version=version)\n",
    "    true = seg.iloc[-1].time_left\n",
    "    pred = predict(seg, synch_gp, route_n, seg_n, true, plot=True)\n",
    "    mae = float(np.abs(true-pred))\n",
    "    mape = float(np.abs((true-pred)/true))\n",
    "    print('true model', route_n, traj_n, seg_n)\n",
    "    print('pred:', f\"{pred:.2f}\", 'true:', float(true), 'MAE:', f\"{mae:.2f}\", 'MAPE:', f\"{mape:.2f}\")\n",
    "    \n",
    "    sns.scatterplot(data=seg, x='lat', y='lon', ax=axs[0, 0])\n",
    "    axs[0, 0].set_title('The new trajectory')\n",
    "    axs[0, 0].set_aspect('equal', 'datalim')\n",
    "    center_axs(axs[0, 0], seg, ['lat', 'lon'])\n",
    "    \n",
    "    synch_gp.model.plot(ax=axs[1,1])\n",
    "    axs[1,1].set_title('Synchronisation GP for the new trajectory segment')\n",
    "    if true_pred_gp:\n",
    "        true_pred_gp.model.plot(ax=axs[2, 0])\n",
    "        axs[2,0].set_title('The true prediction GP')\n",
    "    \n",
    "#print(test_seg[['lat', 'lon']])\n",
    "#pred_gp = gp.load(PREDICTION_MODEL, route_n, traj_n, seg_n)\n",
    "#predict_seg_with_plots(test_seg, route_n, seg_n, true_pred_gp=pred_gp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evaluated 50 out of 720 trajs'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-d1807c3fd22a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraj_start\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraj_start\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_left\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynch_gp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mmae\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseg_n\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraj_n\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mmape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseg_n\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraj_n\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-40a2de1e9310>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(traj, synch_gp, route_n, seg_n, arrival_time, plot)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynch_gp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrival_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mmean_var_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmogp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynch_gp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Don't use all routes. The later ones have crazy small weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# leading to numerical issues.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-40a2de1e9310>\u001b[0m in \u001b[0;36mmogp\u001b[1;34m(traj, synch_gp, route_n, seg_n, plot)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msynched_traj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynch_traj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynch_gp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mlik_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_trajs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLIKELIHOOD_MODEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mlik_models_with_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweighted_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynched_traj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlik_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[0mpred_models_with_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorresponding_pred_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlik_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlik_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlik_models_with_weights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m#print('Most similair model', lik_models_with_weights[0][0].route_n, lik_models_with_weights[0][0].traj_n, lik_models_with_weights[0][0].seg_n)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-40a2de1e9310>\u001b[0m in \u001b[0;36mweighted_models\u001b[1;34m(synched_traj, models)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynched_traj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tau'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynched_traj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mmodels_with_loglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loglik\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mmodels_by_loglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_with_loglik\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mmw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mmax_loglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels_by_loglik\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-40a2de1e9310>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynched_traj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tau'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynched_traj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mmodels_with_loglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loglik\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mmodels_by_loglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_with_loglik\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mmw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mmax_loglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels_by_loglik\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-40a2de1e9310>\u001b[0m in \u001b[0;36mdata_loglik\u001b[1;34m(model, X, Y)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloglik\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-40a2de1e9310>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloglik\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-40a2de1e9310>\u001b[0m in \u001b[0;36mloglik\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdata_loglik\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mloglik\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\code\\tdde19\\det-ar-lugnt\\GP\\gp_gpy.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(gp, X)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mWraps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mGPy\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mScales\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mpredicting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \"\"\"\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mX_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mY_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpflow\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y, copy)\u001b[0m\n\u001b[0;32m    742\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,\n\u001b[0;32m    743\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmay_share_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "test = pd.read_pickle('progress_test.pkl')\n",
    "seg_sizes = [.2, .4, .6, .8]\n",
    "n_segs = test.seg.unique()\n",
    "segs = dict(tuple(test.groupby('seg')))\n",
    "mape = {s: defaultdict(dict) for s in seg_sizes}\n",
    "mae =  {s: defaultdict(dict) for s in seg_sizes}\n",
    "for seg_n in n_segs[1:50]:\n",
    "    seg = segs[seg_n]\n",
    "    n_trajs = seg.traj.unique()\n",
    "    trajs = dict(tuple(seg.groupby('traj')))\n",
    "    for i, traj_n in enumerate(n_trajs):\n",
    "        traj = trajs[traj_n]\n",
    "        for size in seg_sizes:\n",
    "            n = round(traj.shape[0]*size)\n",
    "            traj_start = traj[:n]\n",
    "            synch_gp = gp.load_synch(route_n, seg_n, version=version)\n",
    "            X = traj_start[['lat', 'lon']]\n",
    "            y = traj_start.iloc[-1].time_left\n",
    "            pred = predict(X, synch_gp, route_n, int(seg_n), y, plot=False)\n",
    "            mae[size][seg_n][traj_n]  = float(np.abs(y-pred))\n",
    "            mape[size][seg_n][traj_n] = float(np.abs((y-pred)/y))\n",
    "    \n",
    "        clear_output(wait=True)\n",
    "        display('Evaluated ' + str(i) + ' out of ' + str(len(n_trajs)) + ' trajs')\n",
    "display('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
