{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrival Time Prediction - Pre Processing\n",
    "This notebook is the first on the work on GP regression for arrival time prediction and deals with the pre processing before a model can be trained to make predictions.\n",
    "\n",
    "First off lets get some libraries into scope and load the data. Note that we are ignoring the first segment in the trajectory, since the model has major issues with it. This is because the bus drives around in a small turn on the parking lot, and because it has a very long idle time before it starts its journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import plot\n",
    "\n",
    "data = pd.read_csv('../../bus203_all.csv')\n",
    "data = data[(data.segment_number > 1) & (data.journey_number < 10)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names in the csv are very verbose, so we'll shorten them. This is of course a matter of preference but I prefer it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.rename(columns = {\n",
    "                        'latitude': 'lat', \n",
    "                        'longitude': 'lon', \n",
    "                        'journey_number': 'traj', \n",
    "                        'segment_number': 'seg', \n",
    "                        'speed': 'speed',\n",
    "                        'event': 'event',\n",
    "                        'timestamp': 'timestamp'\n",
    "                    })\n",
    "data.head()\n",
    "data = data.sort_values(['traj', 'seg', 'timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Compression\n",
    "There are many times where busses stop or drive very slowly. This causes a lot of data points to be clustered which will cause the GPs to prioritise getting the clustered areas right more than other areas, which we do not want. To prevent this we will filter out data points in every trajectory that are too close by some delta. However simply throwing the points aray will leave massive gaps in between them, which will not work at all for a GP with constant kernel lengthscale (which we will use). To combat this we will compress the data points during stops to a single data point which takes the mean value of all points during a stop.\n",
    "\n",
    "Worth noting is that coordinates are not in euclidian space but on a spherical surface so pythagoras theorem is no good here and we have to turn to haversine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt, isnan\n",
    " \n",
    "def parse_time(dt_str):\n",
    "    dt, _, _ = dt_str.partition(\".\")\n",
    "    return datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%S\")\n",
    " \n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "   Calculate the great circle distance between two points\n",
    "   on the earth (specified in decimal degrees)\n",
    "   \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    " \n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "    return c * r\n",
    " \n",
    "def as_dict(d):\n",
    "    return {\n",
    "        'lat': d.lat,\n",
    "        'lon': d.lon,\n",
    "        'seg': d.seg,\n",
    "        'speed': d.speed,\n",
    "        'traj': d.traj,\n",
    "        'timestamp': d.timestamp,\n",
    "        'event': d.event\n",
    "    }\n",
    " \n",
    " \n",
    "# I have no idea why, but this consistently returns one hour too much.\n",
    "# Also, we actually call to_datetime twice (one more time outside of the function)\n",
    "# which seems to be needed. No clue why this is the case.\n",
    "def mean_timestamp(timestamps):\n",
    "    dt = pd.to_datetime(timestamps.dropna().astype(np.int64).mean())\n",
    "    dt = dt - pd.Timedelta(hours=1)\n",
    "    return dt\n",
    "\n",
    "def compress(data):\n",
    "    dict_data = [as_dict(x) for x in data]\n",
    "   \n",
    "    if len(dict_data) == 1: return dict_data[0]\n",
    "   \n",
    "    df = pd.DataFrame(dict_data)\n",
    "   \n",
    "    df.speed = np.max(df.speed, 0) # data contains -1 sentinel values for missing speed\n",
    " \n",
    "    df2 = df.drop(['timestamp', 'event', 'seg', 'speed'], axis=1).apply(np.mean, axis=0)\n",
    "    df2['timestamp'] = mean_timestamp(df['timestamp'].apply(parse_time))\n",
    "    df2['timestamp'] = pd.to_datetime(df2['timestamp'])\n",
    "    df2['speed'] = np.mean(df.speed[df.speed >= 0])\n",
    "    contains_entered_event = lambda df : df.event.transform(lambda e : e == 'EnteredEvent').any()\n",
    "    df2['event'] = 'EnteredEvent' if contains_entered_event(df) else 'ObservedPositionEvent'\n",
    "    df2['seg'] = df.seg.min() # In the case of overlapping segments we let the data belong to the first\n",
    " \n",
    "    return as_dict(df2)\n",
    "   \n",
    "t0 = time.time()\n",
    "delta = 4e-3 # approx. 4 metres\n",
    "output = []\n",
    "buffer = [ data.iloc[0] ]\n",
    "for current in data.itertuples():\n",
    "    if current.Index == 0: continue\n",
    "       \n",
    "    distance = haversine(current.lat, current.lon, buffer[-1].lat, buffer[-1].lon)\n",
    "   \n",
    "    if distance > delta:\n",
    "        output.append(compress(buffer))\n",
    "        buffer.clear()\n",
    "   \n",
    "    buffer.append(current)\n",
    " \n",
    "if len(buffer) > 0: \n",
    "    output.append(compress(buffer))\n",
    " \n",
    "compressed_data = pd.DataFrame(output)\n",
    " \n",
    "elapsed = time.time() - t0\n",
    "print(\"Data processed in\", elapsed, \" seconds\")\n",
    "compressed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing arrival times\n",
    "The goal is to estimate arrival time given a trajectory, but to do that we first need to annotate the trajectories in the data with the actual arrival times. In this case we are going to add a field for \"time remaining\". In addition we are going to need to know the _progression_ of each trajectory, which is computed by normalising over the indicies of a trajectory. This is needed to train a synchronisation GP before making the predictions. Observe that this is done after stop compression, which means that we assume that the information lost on how long a trajectory is spatially stationary is irrelevant for our predictions. The following chunk annotates the data with time remaining and normalised temporal progression. Beware that this takes a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "time_left = pd.DataFrame({'time_left': np.zeros(compressed_data.shape[0])})\n",
    "progress = pd.DataFrame({'progress': np.zeros(compressed_data.shape[0])})\n",
    "reverse_rows = compressed_data.iloc[::-1]\n",
    "last_stop_timestamp = reverse_rows.iloc[0].timestamp\n",
    "trajs = compressed_data.traj.unique()\n",
    "segs = compressed_data.seg.unique()\n",
    "\n",
    "sdata = compressed_data.sort_values(['traj', 'seg', 'timestamp'])\n",
    "cur_traj = 0\n",
    "cur_seg = 0\n",
    "tn = 0\n",
    "seg0 = 0\n",
    "segn = 0\n",
    "traj = sdata\n",
    "for i, d in sdata.iterrows():\n",
    "    if d.traj > cur_traj:\n",
    "        cur_seg = 0\n",
    "        cur_traj = d.traj\n",
    "        traj = sdata[sdata.traj == cur_traj]\n",
    "        \n",
    "    if d.seg > cur_seg:\n",
    "        cur_seg = d.seg\n",
    "        seg0 = i\n",
    "        seg = traj[traj.seg == cur_seg]\n",
    "        segn = seg.shape[0]\n",
    "        tn = seg.iloc[-1].timestamp\n",
    "                      \n",
    "    time_left.iloc[i] = (tn - d.timestamp).seconds\n",
    "    progress.iloc[i]  = (i - seg0) / (segn - 1)\n",
    "\n",
    "progress_data = pd.concat([\n",
    "            compressed_data[['lat', 'lon', 'traj', 'seg', 'speed']],\n",
    "            time_left,\n",
    "            progress],\n",
    "            axis = 1)\n",
    "progress_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a plot of the synchronised progression and spatial trajectory to give some feeling of what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_to_plot = progress_data[progress_data.traj == 5]\n",
    "plot.traj_progress(traj_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot.traj_segments(traj_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforce smoothness\n",
    "A property that we want from the synchroinisation GP that we will train is for it to be a smooth mapping with respect to spatial progress, that is, we want regions that are close progressionwise in the functions domain to be close in its codomain. This is not something a GP guarantees at all, so to force it to learn something closer to what we want, extra data is generated by drawing from a normal distribution which is orthogonal to the spatial progression in each data point. By letting these points take on the same progression value we force the GP to not deviate. We call these data points support data points, and they are saved in their own data set.\n",
    "\n",
    "This is meaningful only for training data, so at this point we split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trajs = len(progress_data.traj.unique())\n",
    "n_train = round(n_trajs*0.8)\n",
    "train_ixs = frozenset(np.random.randint(0, n_trajs-1, n_train))\n",
    "\n",
    "train_data = progress_data[progress_data.traj.transform(lambda x: x in train_ixs)]\n",
    "test_data = progress_data[progress_data.traj.transform(lambda x: x not in train_ixs)]\n",
    "train_data.to_pickle('train.pkl')\n",
    "test_data.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "tmp_data = []\n",
    "N = train_data.shape[0]\n",
    "t0 = time.time()\n",
    "d = 5 # amount of random draws\n",
    "\n",
    "def move_to(data, lat, lon):\n",
    "    data.lat = lat\n",
    "    data.lon = lon\n",
    "    return data\n",
    "\n",
    "for n in range(N-1):\n",
    "    cur = train_data.iloc[n]\n",
    "    nxt = train_data.iloc[n+1]\n",
    "    d_lat = nxt.lat - cur.lat\n",
    "    d_lon = nxt.lon - cur.lon\n",
    "    u = np.array([-d_lon, d_lat]) # orthogonal to progression \n",
    "    u = u/norm(u)\n",
    "    \n",
    "    v = np.array([cur.lat, cur.lon]) # current data point position\n",
    "    sigma = 9e-5 # taken by simply looking at the results for something that looks decent\n",
    "    draws = np.random.normal(0, sigma, d)\n",
    "    support_latlon = [v + u*draw for draw in draws]\n",
    "    \n",
    "    support_data = [move_to(cur.copy(), lat, lon) for lat, lon in support_latlon]\n",
    "    tmp_data = tmp_data + support_data\n",
    "    \n",
    "support_data = pd.DataFrame(tmp_data)\n",
    "support_data.to_pickle('support.pkl')\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(\"Data processed in\", elapsed, \" seconds\")\n",
    "\n",
    "traj_plot_n = 5\n",
    "traj_to_plot = progress_data[progress_data.traj == traj_plot_n]\n",
    "sup_to_plot = support_data[support_data.traj == traj_plot_n]\n",
    "plot.traj_segment_grid(support_data, 'lat', 'lon')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
